{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# AY22 Artifical Intelligence (A) - Assignment 2022-2023\n",
    "Chest X-ray Abnormalities Detection using Deep Learning: Development of a deep learning model to automatically localize and classify thoracic abnormalities from chest radiographs. (Option n°2)\n",
    "\n",
    "© Copyright 2023, All rights reserved to Hans Haller, CSTE-CIDA Student at Cranfield Uni. SATM, Cranfield, UK.\n",
    "\n",
    "https://www.github.com/Hnshlr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOOGLE COLAB OPTIONS (IGNORE THES STEPS IF NOT ON COLAB):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp '/content/drive/MyDrive/CRANFIELD/9_AI/AI_ASSIGNMENT/AI_CODE_HALLER_388885/src.zip' '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "!unzip /content/src.zip -d /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/content/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/ultralytics/yolov5\n",
    "!cd /content/src/yolostuff && git clone https://github.com/ultralytics/yolov5  # clone repo\n",
    "# %cd yolov5\n",
    "!cd /content/src/yolostuff/yolov5 && pip install -r requirements.txt  # install dependencies\n",
    "# cmd = \"!python {yolo_dir}train.py --img 256 --batch 32 --epochs 2 --data {yaml_path} --weights {model} --cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS=\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pydicom as dicom\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# MY DATASET=\n",
    "from src import ChestXrayDataset as CXD\n",
    "from src.Preprocessing import *\n",
    "from src.Postprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTINGS =\n",
    "data_dir = \"src/data/input/256x256/\"                            # MAIN DIRECTORY CONTAINING THE DATA\n",
    "train_df = pd.read_csv(data_dir + \"train.csv\")                  # TRAINING DATA\n",
    "train_df_sizes = pd.read_csv(data_dir + \"train_meta.csv\")       # TRAINING DATA SIZES\n",
    "\n",
    "# ADV. SETTINGS =\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps else \"cpu\")   # DO NOT TOUCH\n",
    "\n",
    "# PREPROCESSING:\n",
    "new_size = 256  # NEW SIZE OF THE IMAGES\n",
    "train_df = preprocess_data(data_dir, train_df, train_df_sizes, new_size)  # DATA PREPROCESSING (+ SAVE -> train_clean.csv)\n",
    "class_ids, class_names = class_ids_and_names(train_df)  # CLASS IDS AND NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIRST APPROACH: RESNET18 FULL IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMATION PIPELINE:\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize the image\n",
    "])\n",
    "\n",
    "# DATASET:\n",
    "dataset = CXD.ChestXrayDataset(csv_file=\"train_single.csv\", data_dir=data_dir, transform=transform) # Create the dataset\n",
    "\n",
    "# TRAIN/VALIDATION SPLIT:\n",
    "ratio = 0.8\n",
    "train_dataset, val_dataset = dataset.split(ratio)   # Split the dataset using the ratio\n",
    "\n",
    "# DATALOADERS:\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of train images: \", len(train_dataset))\n",
    "print(\"Number of validation images: \", len(val_dataset))\n",
    "print(\"Number of batches: \", len(train_loader))\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    print(\"Images shape: \", images.shape)\n",
    "    print(\"Labels shape: \", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the model:\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_classes = 15\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_classes) # Fully connected layer\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and the optimizer:\n",
    "criterion = nn.CrossEntropyLoss()   # Loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Start a timer:\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model:\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    model.train()   # Set the model to training mode\n",
    "    print(\"Epoch: \", epoch)\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # if i % 10 == 0:\n",
    "            # print(\"Batch: \"+str(i)+\" began.\")\n",
    "        images = images.to(device)  # Move the images to the device\n",
    "        labels = labels.to(device)  # Move the labels to the device\n",
    "        optimizer.zero_grad()   # Zero the gradients\n",
    "        outputs = model(images) # Forward pass\n",
    "        loss = criterion(outputs, labels)   # Loss calculation\n",
    "        loss.backward()     # Weight update\n",
    "        optimizer.step()    # Gradient update\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs.data, 1)   # Get the predictions\n",
    "        train_correct += torch.sum(preds == labels.data).sum().item()   # Calculate the number of correct predictions\n",
    "        train_loss += loss.item() * images.size(0)  # Update the training loss\n",
    "    train_acc = train_correct / len(train_dataset)  # Calculate the training accuracy\n",
    "    train_loss = train_loss / len(train_dataset)    # Calculate the training loss\n",
    "    print(\"Epoch: {}/{}...\".format(epoch + 1, num_epochs),\n",
    "          \"Training Loss: {:.4f}...\".format(train_loss),\n",
    "          \"Training Accuracy: {:.4f}\".format(train_acc))\n",
    "\n",
    "# Stop the timer:\n",
    "end_time = time.time()\n",
    "print(\"Resnet18 training took: \", np.round((end_time - start_time), 2), \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model:\n",
    "# torch.save(model.state_dict(), \"src/data/output/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Load the model:\n",
    "model.load_state_dict(torch.load(\"src/data/output/resnet18_e10.pth\"))\n",
    "# Then, re-test the model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Test the model on the validation set:\n",
    "model.eval()\n",
    "val_loss = 0.0  # Validation loss\n",
    "val_correct = 0 # Number of correct predictions\n",
    "for i, (images, labels) in enumerate(val_loader):\n",
    "    images = images.to(device)  # Move the images to the device\n",
    "    labels = labels.to(device)  # Move the labels to the device\n",
    "    outputs = model(images)     # Forward pass\n",
    "    loss = criterion(outputs, labels)   # Loss calculation\n",
    "    val_loss += loss.item() * images.size(0)    # Update the validation loss\n",
    "    _, preds = torch.max(outputs.data, 1)   # Get the predictions\n",
    "    val_correct += torch.sum(preds == labels.data).sum().item()  # Calculate the number of correct predictions\n",
    "val_acc = val_correct / len(val_dataset)    # Calculate the validation accuracy\n",
    "val_loss = val_loss / len(val_dataset)  # Calculate the validation loss\n",
    "print(\"Validation Loss: {:.4f}...\".format(val_loss),\n",
    "      \"Validation Accuracy: {:.4f}\".format(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEW MODEL: YOLOv5 (CLONED FROM GITHUB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/ultralytics/yolov5\n",
    "# !git clone https://github.com/ultralytics/yolov5  # clone repo\n",
    "# %cd yolov5\n",
    "# !pip install -r requirements.txt  # install dependencies\n",
    "# cmd = \"!python {yolo_dir}train.py --img 256 --batch 32 --epochs 2 --data {yaml_path} --weights {model} --cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO STUFF:\n",
    "yolostuff_dir = \"src/yolostuff/\"\n",
    "yolo_dir = yolostuff_dir + \"yolov5/\"\n",
    "yaml_path = yolostuff_dir + \"datasets/vinbigdata/vinbigdata.yaml\"\n",
    "model_path = yolostuff_dir + \"yolov5/models/yolov5s.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.8\n",
    "val_df = train_df.sample(frac=1-ratio, random_state=42)\n",
    "train_df = train_df.drop(val_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the images names in a .txt:\n",
    "txt_file = \"\"\n",
    "for row in train_df[\"image_id\"]:\n",
    "    txt_file += \"./images/\" + row + \".png\\n\"\n",
    "txt_file_path = yolostuff_dir + \"datasets/vinbigdata/train.txt\"\n",
    "txt_file_opened = open(txt_file_path, \"w\")\n",
    "txt_file_opened.write(txt_file)\n",
    "txt_file_opened.close()\n",
    "\n",
    "# Save all the images names in a .txt:\n",
    "txt_file = \"\"\n",
    "for row in val_df[\"image_id\"]:\n",
    "    txt_file += \"./images/\" + row + \".png\\n\"\n",
    "txt_file_path = yolostuff_dir + \"datasets/vinbigdata/val.txt\"\n",
    "txt_file_opened = open(txt_file_path, \"w\")\n",
    "txt_file_opened.write(txt_file)\n",
    "txt_file_opened.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python {yolo_dir}train.py --img 256 --batch 32 --epochs 1 --data {yaml_path} --weights {model_path} --cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v5s_e30 = 'src/yolostuff/yolov5/runs/train/exp_v5s_e30/weights/best.pt'\n",
    "v5x_e30 = 'src/yolostuff/yolov5/runs/train/exp_v5x_e30/weights/best.pt'\n",
    "v5x_e60 = 'src/yolostuff/yolov5/runs/train/exp_v5x_e60/weights/best.pt'\n",
    "model_path = v5x_e60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python {yolo_dir}detect.py --weights {model_path} --img 256 --conf 0.15 --iou 0.5 --source 'src/data/input/256x256/test' --save-txt --save-conf --exist-ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POST-PROCESSING OF THE YOLOv5 OUTPUTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_labels_path = \"src/yolostuff/yolov5/runs/detect/exp/labels/\"\n",
    "test_meta_path = \"src/data/input/256x256/test_meta.csv\"\n",
    "sample_submission_path = \"src/data/input/sample_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = postprocess_yolo(output_labels_path, test_meta_path, sample_submission_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Plot one image, its labels and its bounding boxes:\n",
    "batch1 = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "index = 10\n",
    "img = batch1[0][index].permute(1, 2, 0)\n",
    "# In this format, the image is in RGB, but the values are between -1 and 1.\n",
    "# We need to convert it to 0-255:\n",
    "img = (img + 1) / 2\n",
    "plt.imshow(img)\n",
    "label = batch1[1][index].item()\n",
    "bbox = batch1[2][index]\n",
    "# Get first element of bbox, and convert it to a int:\n",
    "print(bbox)\n",
    "print(label)\n",
    "# Debug: Plot the bounding boxes:\n",
    "for i in range(0, len(bbox), 4):\n",
    "    xmin = bbox[i].item()\n",
    "    ymin = bbox[i+1].item()\n",
    "    width = bbox[i+2].item()\n",
    "    height = bbox[i+3].item()\n",
    "    rect = plt.Rectangle((xmin, ymin), width, height, fill=False, color='red')\n",
    "    plt.gca().add_patch(rect)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "test_img_id = '013c169f9dad6f1f6485da961b9f7bf2'\n",
    "test_img = cv2.imread(\"src/data/input/256x256/test/\" + test_img_id + \".png\")\n",
    "test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(test_img)\n",
    "test_img_predLabels = pd.read_csv(\"src/yolostuff/yolov5/runs/detect/exp/labels/\" + test_img_id + \".txt\", header=None)\n",
    "# Keep the first row:\n",
    "values = test_img_predLabels.iloc[0]\n",
    "# Get the values:\n",
    "values = values[0].split(\" \")\n",
    "x_mid_n = float(values[1])*256\n",
    "y_mid_n = float(values[2])*256\n",
    "width_n = float(values[3])*256\n",
    "height_n = float(values[4])*256\n",
    "x_min_n = x_mid_n - width_n / 2\n",
    "y_min_n = y_mid_n - height_n / 2\n",
    "x_max_n = x_mid_n + width_n / 2\n",
    "y_max_n = y_mid_n + height_n / 2\n",
    "# Plot a rectangle above the image (dont resize):\n",
    "plt.gca().add_patch(plt.Rectangle((x_min_n, y_min_n), width_n, height_n, fill=False, color='red'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Acknowledgements\n",
    "\n",
    "This code was developed as part of the Artificial Intelligence course at Cranfield University, UK."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
