{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS=\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pydicom as dicom\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# MY DATASET=\n",
    "from src import ChestXrayDataset as CXD\n",
    "from src import ChestXrayDatasetV2 as CXD2\n",
    "from src.Preprocessing import preprocess_data\n",
    "from src.Preprocessing import class_ids_and_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTINGS =\n",
    "data_dir = \"src/data/input/256x256/\"                            # MAIN DIRECTORY CONTAINING THE DATA\n",
    "train_df = pd.read_csv(data_dir + \"train.csv\")                  # TRAINING DATA\n",
    "train_df_sizes = pd.read_csv(data_dir + \"train_meta.csv\")       # TRAINING DATA SIZES\n",
    "\n",
    "# ADV. SETTINGS =\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps else \"cpu\")   # DO NOT TOUCH\n",
    "\n",
    "# PREPROCESSING:\n",
    "train_df = preprocess_data(data_dir, train_df, train_df_sizes)  # DATA PREPROCESSING (+ SAVE -> train_clean.csv)\n",
    "class_ids, class_names = class_ids_and_names(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### FIRST APPROACH: RESNET18 FULL IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMATION PIPELINE:\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize the image\n",
    "])\n",
    "\n",
    "# DATASET:\n",
    "dataset = CXD.ChestXrayDataset(csv_file=\"train_clean.csv\", data_dir=data_dir, transform=transform)\n",
    "\n",
    "# TRAIN/VALIDATION SPLIT:\n",
    "ratio = 0.8\n",
    "train_dataset, val_dataset = dataset.split(ratio)\n",
    "\n",
    "# DATALOADERS:\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of train images: \", len(train_dataset))\n",
    "print(\"Number of validation images: \", len(val_dataset))\n",
    "print(\"Number of batches: \", len(train_loader))\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    print(\"Images shape: \", images.shape)\n",
    "    print(\"Labels shape: \", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model:\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_classes = 15\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and the optimizer:\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model:\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    model.train()   # Set the model to training mode\n",
    "    print(\"Epoch: \", epoch)\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        if i % 10 == 0:\n",
    "            print(\"Batch: \"+str(i)+\" began.\")\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()     # Weight update\n",
    "        optimizer.step()    # Gradient update\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        train_correct += torch.sum(preds == labels.data).sum().item()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    train_acc = train_correct / len(train_dataset)\n",
    "    train_loss = train_loss / len(train_dataset)\n",
    "    print(\"Epoch: {}/{}...\".format(epoch + 1, num_epochs),\n",
    "          \"Training Loss: {:.4f}...\".format(train_loss),\n",
    "          \"Training Accuracy: {:.4f}\".format(train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on the validation set:\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "val_correct = 0\n",
    "for i, (images, labels) in enumerate(val_loader):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    val_loss += loss.item() * images.size(0)\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    val_correct += torch.sum(preds == labels.data).sum().item()\n",
    "val_acc = val_correct / len(val_dataset)\n",
    "val_loss = val_loss / len(val_dataset)\n",
    "print(\"Validation Loss: {:.4f}...\".format(val_loss),\n",
    "      \"Validation Accuracy: {:.4f}\".format(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model:\n",
    "# torch.save(model.state_dict(), \"src/data/output/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model:\n",
    "model.load_state_dict(torch.load(\"src/data/output/resnet18_e10.pth\"))\n",
    "# Then, re-test the model on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### NEW MODEL: YOLOv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/ultralytics/yolov5\n",
    "# !git clone https://github.com/ultralytics/yolov5  # clone repo\n",
    "# %cd yolov5\n",
    "# !pip install -r requirements.txt  # install dependencies\n",
    "# cmd = \"!python {yolo_dir}train.py --img 256 --batch 32 --epochs 2 --data {yaml_path} --weights {model} --cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.8\n",
    "val_df = train_df.sample(frac=1-ratio, random_state=42)\n",
    "train_df = train_df.drop(val_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO STUFF:\n",
    "yolostuff_dir = \"src/yolostuff/\"\n",
    "yaml_path = yolostuff_dir + \"datasets/vinbigdata/vinbigdata.yaml\"\n",
    "model = yolostuff_dir + \"yolov5/models/yolov5s.pt\"\n",
    "\n",
    "# Save all the images names in a .txt:\n",
    "txt_file = \"\"\n",
    "for row in train_df[\"image_id\"]:\n",
    "    txt_file += \"./images/\" + row + \".png\\n\"\n",
    "txt_file_path = yolostuff_dir + \"datasets/vinbigdata/train.txt\"\n",
    "txt_file_opened = open(txt_file_path, \"w\")\n",
    "txt_file_opened.write(txt_file)\n",
    "txt_file_opened.close()\n",
    "\n",
    "# Save all the images names in a .txt:\n",
    "txt_file = \"\"\n",
    "for row in val_df[\"image_id\"]:\n",
    "    txt_file += \"./images/\" + row + \".png\\n\"\n",
    "txt_file_path = yolostuff_dir + \"datasets/vinbigdata/val.txt\"\n",
    "txt_file_opened = open(txt_file_path, \"w\")\n",
    "txt_file_opened.write(txt_file)\n",
    "txt_file_opened.close()\n",
    "\n",
    "# Save all the images names in a .txt:\n",
    "test_df = pd.read_csv(\"src/data/input/256x256/sample_submission.csv\")\n",
    "# TODO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python {yolo_dir}train.py --img 256 --batch 32 --epochs 1 --data {yaml_path} --weights {model} --cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python {yolo_dir}detect.py --weights 'src/yolostuff/yolov5/runs/train/exp19/weights/best.pt' --img 256 --conf 0.15 --iou 0.5 --source 'src/data/input/256x256/test' --exist-ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Plot one image, its labels and its bounding boxes:\n",
    "batch1 = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 10\n",
    "img = batch1[0][index].permute(1, 2, 0)\n",
    "# In this format, the image is in RGB, but the values are between -1 and 1.\n",
    "# We need to convert it to 0-255:\n",
    "img = (img + 1) / 2\n",
    "plt.imshow(img)\n",
    "label = batch1[1][index].item()\n",
    "bbox = batch1[2][index]\n",
    "# Get first element of bbox, and convert it to a int:\n",
    "print(bbox)\n",
    "print(label)\n",
    "# Debug: Plot the bounding boxes:\n",
    "for i in range(0, len(bbox), 4):\n",
    "    xmin = bbox[i].item()\n",
    "    ymin = bbox[i+1].item()\n",
    "    width = bbox[i+2].item()\n",
    "    height = bbox[i+3].item()\n",
    "    rect = plt.Rectangle((xmin, ymin), width, height, fill=False, color='red')\n",
    "    plt.gca().add_patch(rect)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
